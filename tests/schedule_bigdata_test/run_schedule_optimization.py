# tests/schedule_bigdata_test/run_schedule_optimization.py
# -*- coding: utf-8 -*-
"""
å°ˆæ¡ˆåç¨±ï¼šASBL-Basketball-Manager
æ¨¡çµ„åç¨±ï¼šè³½ç¨‹ç©åˆ†å¤§æ•¸æ“šåˆ†æå·¥å…· (V2 - Parquet Recorder)
åŠŸèƒ½æè¿°ï¼š
    1. åŸ·è¡Œè’™åœ°å¡ç¾…æ¨¡æ“¬åˆ†æè³½ç¨‹ç©åˆ†ã€‚
    2. æ”¯æ´å¤šé€²ç¨‹ (Multiprocessing) ä¸¦è¡Œé‹ç®—ã€‚
    3. å¯¦ä½œ tqdm é€²åº¦æ¢é¡¯ç¤ºã€‚
    4. å°‡æ‰€æœ‰éš¨æ©Ÿç”Ÿæˆçš„ã€Œè³½ç¨‹çµ„åˆ(Indices)ã€èˆ‡ã€Œç©åˆ†(Score)ã€åˆ†æ‰¹å¯«å…¥ Parquet æª”æ¡ˆã€‚
"""

import os
import sys
import time
import psutil
import multiprocessing
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
from datetime import datetime
from tqdm import tqdm  # é€²åº¦æ¢å¥—ä»¶

# è¨­å®šä¸­æ–‡å­—å‹
import platform
system_name = platform.system()
if system_name == "Windows":
    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei']
elif system_name == "Darwin":
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Heiti TC']
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# é…ç½®åƒæ•¸
# ==========================================
TOTAL_ITERATIONS = 100_000_000  # ç›®æ¨™ï¼šä¸€å„„æ¬¡
BATCH_SIZE = 100_000            # Worker å–®æ¬¡é‹ç®—é‡
FLUSH_THRESHOLD = 2_000_000     # æ¯ç´¯ç©å¤šå°‘ç­†è³‡æ–™å¯«å…¥ä¸€æ¬¡ç¡¬ç¢Ÿ (æ§åˆ¶è¨˜æ†¶é«”)

NUM_TEAMS = 36
NUM_DAYS = 70

# è¼¸å‡ºç›®éŒ„è¨­å®š
OUTPUT_DIR = "tests/schedule_bigdata_test/data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è‡ªå‹•åµæ¸¬ CPU æ ¸å¿ƒæ•¸ (ä¿ç•™ 2 æ ¸å¿ƒçµ¦ç³»çµ±èˆ‡ I/O)
WORKER_COUNT = max(1, os.cpu_count() - 2)

# ==========================================
# æ ¸å¿ƒé‚è¼¯ (Core Logic)
# ==========================================

def generate_round_robin(num_teams=36, num_days=70):
    """ç”¢ç”Ÿæ¨™æº–åœ“æ¡Œæ³•è³½ç¨‹"""
    schedule = []
    teams = list(range(1, num_teams + 1))
    fixed_team = teams[0]
    rotating_teams = teams[1:]
    
    for day in range(num_days):
        daily_matches = []
        if day % 2 == 0:
            daily_matches.append((fixed_team, rotating_teams[0]))
        else:
            daily_matches.append((rotating_teams[0], fixed_team))
            
        for i in range(1, len(teams) // 2):
            t1 = rotating_teams[i]
            t2 = rotating_teams[-i]
            if day % 2 == 0:
                daily_matches.append((t1, t2))
            else:
                daily_matches.append((t2, t1))
        
        schedule.append(daily_matches)
        rotating_teams = [rotating_teams[-1]] + rotating_teams[:-1]
        print("åœ“æ¡Œç®—æ³•å·²å®Œæˆ")
    return schedule

# é å…ˆç”Ÿæˆè³½ç¨‹ (Global)
BASE_SCHEDULE = generate_round_robin(NUM_TEAMS, NUM_DAYS)

def worker_task(iterations):
    """
    Worker åŸ·è¡Œç·’ï¼š
    å›å‚³: (scores_array, indices_matrix)
    """
    # 1. é‡å»ºæŸ¥æ‰¾è¡¨ (Day -> Team -> Venue 0/1)
    venues_matrix = np.zeros((NUM_DAYS, NUM_TEAMS + 1), dtype=np.int8)
    for d, matches in enumerate(BASE_SCHEDULE):
        for home, away in matches:
            venues_matrix[d][home] = 0 # Home
            venues_matrix[d][away] = 1 # Away
            
    # æº–å‚™å®¹å™¨
    # scores: int16 (ç©åˆ†é€šå¸¸ä¸æœƒè¶…é 32767)
    # indices: int8 (å¤©æ•¸ 0-69)
    local_scores = np.zeros(iterations, dtype=np.int16)
    local_indices = np.zeros((iterations, NUM_DAYS), dtype=np.int8)
    
    day_indices = np.arange(NUM_DAYS, dtype=np.int8)
    
    for i in range(iterations):
        # 2. éš¨æ©Ÿæ‰“äº‚
        np.random.shuffle(day_indices)
        
        # å­˜å…¥ indices (é€™æ˜¯æˆ‘å€‘è¦ç´€éŒ„çš„è³½ç¨‹çµ„åˆ)
        local_indices[i] = day_indices
        
        # 3. è¨ˆç®—ç©åˆ†
        shuffled_venues = venues_matrix[day_indices]
        
        # è½‰ç½®çŸ©é™£ (Team, Day)
        team_matrix = shuffled_venues.T
        total_score = 0
        
        # é‡å° 36 éšŠè¨ˆç®—
        for t in range(1, NUM_TEAMS + 1):
            venues = team_matrix[t]
            # æ‰¾å‡ºè®ŠåŒ–é»
            change_indices = np.where(venues[:-1] != venues[1:])[0] + 1
            boundaries = np.concatenate(([0], change_indices, [NUM_DAYS]))
            lengths = np.diff(boundaries)
            
            # å‘é‡åŒ–è¨ˆåˆ†
            score = 0
            score += np.sum(lengths == 2) * 1
            score += np.sum(lengths == 3) * 3
            score += np.sum(lengths == 4) * 5
            score += np.sum(lengths == 5) * 10
            score += np.sum(lengths >= 6) * 30
            
            total_score += score
        
        local_scores[i] = total_score
        
    return local_scores, local_indices

# ==========================================
# ä¸»ç¨‹å¼ (Main)
# ==========================================

def save_chunk_to_parquet(scores, indices, chunk_id):
    """å°‡æ•¸æ“šå¯«å…¥ Parquet"""
    # å»ºç«‹ DataFrame
    # Columns: score, d0, d1, ..., d69
    cols = ['score'] + [f'd{i}' for i in range(NUM_DAYS)]
    
    # çµ„åˆæ•¸æ“š: å…ˆå°‡ score è½‰ç‚º (N, 1)ï¼Œå†èˆ‡ indices (N, 70) åˆä½µ
    # æ³¨æ„ï¼šscores æ˜¯ int16, indices æ˜¯ int8ï¼Œåˆä½µæ™‚æœƒè®Šæˆ int16
    data = np.hstack((scores.reshape(-1, 1), indices))
    
    df = pd.DataFrame(data, columns=cols)
    
    # ç‚ºäº†ç¯€çœç©ºé–“ï¼Œå¼·åˆ¶è½‰å‹
    # score -> int16, d0~d69 -> int8
    convert_dict = {'score': 'int16'}
    for c in cols[1:]:
        convert_dict[c] = 'int8'
    
    df = df.astype(convert_dict)
    
    filename = os.path.join(OUTPUT_DIR, f"schedule_sim_part_{chunk_id:04d}.parquet")
    df.to_parquet(filename, engine='pyarrow', compression='snappy')
    return filename

def main():
    print("="*60)
    print(f"ğŸš€ ASBL è³½ç¨‹å„ªåŒ–å¤§æ•¸æ“šåˆ†æ (V2 - Parquet Recorder)")
    print(f"ğŸ¯ ç›®æ¨™: {TOTAL_ITERATIONS:,} æ¬¡æ¨¡æ“¬")
    print(f"ğŸ’¾ è¼¸å‡º: {OUTPUT_DIR}/*.parquet")
    print(f"ğŸ’» ç¡¬é«”: {WORKER_COUNT} Workers")
    print("="*60)
    
    # æº–å‚™ä»»å‹™
    num_tasks = TOTAL_ITERATIONS // BATCH_SIZE
    tasks = [BATCH_SIZE] * num_tasks
    remainder = TOTAL_ITERATIONS % BATCH_SIZE
    if remainder > 0:
        tasks.append(remainder)
    
    # æ•¸æ“šç·©è¡å€
    score_buffer = []
    indices_buffer = []
    
    # çµ±è¨ˆç”¨ (åªå­˜åˆ†æ•¸ï¼Œä¸å­˜ Indices ä»¥ç¯€çœè¨˜æ†¶é«”)
    all_scores_history = [] 
    
    chunk_counter = 1
    start_time = time.time()
    
    # å•Ÿå‹•å¤šé€²ç¨‹æ± 
    with multiprocessing.Pool(processes=WORKER_COUNT) as pool:
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
        with tqdm(total=TOTAL_ITERATIONS, unit="sim", desc="æ¨¡æ“¬é€²åº¦") as pbar:
            
            for scores, indices in pool.imap_unordered(worker_task, tasks):
                # 1. æ”¶é›†æ•¸æ“š
                score_buffer.append(scores)
                indices_buffer.append(indices)
                all_scores_history.append(scores) # åƒ…ç”¨æ–¼æœ€å¾Œç¹ªåœ–
                
                batch_len = len(scores)
                pbar.update(batch_len)
                
                # 2. æª¢æŸ¥ç·©è¡å€æ˜¯å¦é”åˆ°å¯«å…¥é–€æª»
                current_buffer_size = sum(len(x) for x in score_buffer)
                
                if current_buffer_size >= FLUSH_THRESHOLD:
                    # åˆä½µç·©è¡å€
                    flush_scores = np.concatenate(score_buffer)
                    flush_indices = np.concatenate(indices_buffer)
                    
                    # å¯«å…¥ç£ç¢Ÿ
                    save_chunk_to_parquet(flush_scores, flush_indices, chunk_counter)
                    
                    # æ›´æ–°ç‹€æ…‹
                    pbar.set_postfix_str(f"Saved Part {chunk_counter}")
                    chunk_counter += 1
                    
                    # æ¸…ç©ºç·©è¡å€
                    score_buffer = []
                    indices_buffer = []
                    
                    # è¨˜æ†¶é«”ç›£æ§ (å¯é¸)
                    # mem = psutil.virtual_memory()
                    # if mem.percent > 90: ...

    # å¯«å…¥å‰©é¤˜çš„æ•¸æ“š
    if score_buffer:
        flush_scores = np.concatenate(score_buffer)
        flush_indices = np.concatenate(indices_buffer)
        save_chunk_to_parquet(flush_scores, flush_indices, chunk_counter)
        print(f"âœ… å·²å¯«å…¥æœ€å¾Œå€å¡Š Part {chunk_counter}")

    total_time = time.time() - start_time
    print(f"\nâœ¨ æ¨¡æ“¬å…¨éƒ¨å®Œæˆï¼ç¸½è€—æ™‚: {total_time:.2f} ç§’")
    
    # --- æ•¸æ“šåˆ†æèˆ‡ç¹ªåœ– ---
    print("\nğŸ“Š æ­£åœ¨é€²è¡Œæ•¸æ“šçµ±è¨ˆåˆ†æ...")
    
    # å°‡æ­·å²åˆ†æ•¸åˆä½µç‚ºä¸€å€‹å¤§é™£åˆ— (æ³¨æ„è¨˜æ†¶é«”ï¼Œè‹¥ 1 å„„å€‹ int16 ç´„ 200MBï¼Œéå¸¸å®‰å…¨)
    final_scores = np.concatenate(all_scores_history)
    
    stats = {
        "min": np.min(final_scores),
        "max": np.max(final_scores),
        "mean": np.mean(final_scores),
        "std": np.std(final_scores),
        "p0.1": np.percentile(final_scores, 0.1),
        "p1": np.percentile(final_scores, 1),
        "p5": np.percentile(final_scores, 5),
        "p50": np.median(final_scores)
    }
    
    print("-" * 40)
    print(f"æœ€ä½ç©åˆ† (Best): {stats['min']}")
    print(f"æœ€é«˜ç©åˆ† (Worst): {stats['max']}")
    print(f"å¹³å‡ç©åˆ†:       {stats['mean']:.2f} (Ïƒ={stats['std']:.2f})")
    print(f"Top 0.1% é–€æª»:  <{stats['p0.1']:.1f}")
    print(f"Top 1% é–€æª»:    <{stats['p1']:.1f}")
    print("-" * 40)
    
    # --- ç¹ªåœ– ---
    print("ğŸ¨ æ­£åœ¨ç¹ªè£½åœ–è¡¨...")
    plt.figure(figsize=(12, 6))
    
    # ç›´æ–¹åœ–
    plt.hist(final_scores, bins=100, density=True, alpha=0.6, color='skyblue', edgecolor='black', label='æ¨¡æ“¬åˆ†ä½ˆ')
    
    # KDE æ›²ç·š (å–æ¨£ç¹ªè£½)
    sample_size = min(100000, len(final_scores))
    sample = np.random.choice(final_scores, sample_size, replace=False)
    kde = gaussian_kde(sample)
    x_grid = np.linspace(stats['min'], stats['max'], 200)
    plt.plot(x_grid, kde(x_grid), 'r-', linewidth=2, label='KDE å¯†åº¦æ›²ç·š')
    
    # æ¨™è¨˜ç·š
    plt.axvline(stats['p1'], color='green', linestyle='--', linewidth=2, label=f'Top 1% (<{stats["p1"]:.0f})')
    plt.axvline(stats['mean'], color='orange', linestyle='--', linewidth=2, label=f'å¹³å‡ ({stats["mean"]:.0f})')
    
    plt.title(f'è³½ç¨‹ç©åˆ†å¸¸æ…‹åˆ†ä½ˆ (N={TOTAL_ITERATIONS:,})\nè€—æ™‚: {total_time:.1f}s', fontsize=14)
    plt.xlabel('ç©åˆ† (è¶Šä½è¶Šå¥½)', fontsize=12)
    plt.ylabel('æ©Ÿç‡å¯†åº¦', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    output_img = f'schedule_analysis_{datetime.now().strftime("%Y%m%d_%H%M")}.png'
    plt.savefig(output_img)
    print(f"ğŸ’¾ åœ–è¡¨å·²å„²å­˜è‡³: {output_img}")
    plt.show()

if __name__ == '__main__':
    multiprocessing.freeze_support()
    main()